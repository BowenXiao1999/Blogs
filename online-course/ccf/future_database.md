# 数据库研究方向

很多人觉得数据库发展这么久，理论如此成熟，科研里面可以做的东西少了。其实不然，在当下数据越来越爆炸的时代，挑战与机会是并存的。本文是对Guoliang Li老师在CCF走进校园的报告的一份总结，供个人学习。

先回顾一下数据库的架构。从单体数据库，到主备来做高可用，再到Oracle RAC的多活，再到完全分布式架构的数据库。RAC这种类型支持了多活，但是却在存储容量上受限，并且有一个很重要的问题(cache一致性)。cache一致性是指，更新操作一般是WAL+内存来避免同步随机写，那这样一来就带来一个问题，A可能把更新数据存在内存里了，还没来得及写入磁盘，但是B这时候收到一个读相关数据的请求，要怎么样保证能读到最新的数据？

分布式数据库讲究shard，并且同样是多活。但是这带来很多问题，比如事务模型怎么做，查询优化怎么做。

## 分布式数据库的挑战

### 分片
分布式数据库，一个很重要的关键点就是数据怎么分的问题。行存还是列存？这个比较好选择。有更多问题等着研究，比如

* 如何保证你选的partition key在数据增长的时候依然保持均匀分布？
* 如何选择Partition的列？是不是应该在Query的比较多的列上做Partition？
* Join的比较频繁的2个Region，是不是一般放到一个节点比较好？（跨Region查询）

这里有人在[SIGMOD](https://dl.acm.org/doi/10.1145/3318464.3389704)上用学习的办法去学partition应该怎么做。同样还有Graph Key Partition。这里没听的特别明白，不过我觉得确实也大有可为，比如你本身就是Graph Database，那么用Graph Key Partition也就很符合数据特性。或者说你用图的办法建模，例如每个节点表示一列，这样可以把频繁Join或者做事务的2个Region放到一个节点上。

以及还有一个研究关键点，partiton完了之后，怎么Evaluate我的partition方法好不好？因为很有可能可以分片的方案非常多，不可能做到每一种新的Partition都完整跑SQL Query来评测。因此有没有一种Evaluate Model，我给出一个分片方案，就能以比较数学的方式来评估方案的表现？我觉得如果有了这么一个优秀模型，对于[Automatically Partition](https://hstore.cs.brown.edu/papers/hstore-partitioning.pdf)领域应该是一个非常好的助推剂。

### 事务
分布式的事务支持，是Newsql和分库分表方案的关键点。有很多业务至今没有使用Newsql，就是因为它们的事务模型比较简单，分库分表就能解决。因此，这个模块怎么设计也是一大难题。

业界普遍使用的比较多的模型是2PC。但是2PC有很多种实现方式，比如各种优化。事务模型涉及到的概念比较多，比如怎么做并发控制，怎么控制写写冲突（一般通过锁），读写冲突（MVCC）。同时，数据库要支持哪些隔离级别也非常影响相关的设计。比较传统的方案，应该是通过统一发号器，每次查询前/事务开始前拿到一致性视图。PG在08年的[论文](https://courses.cs.washington.edu/courses/cse444/08au/544M/READING-LIST/fekete-sigmod2008.pdf)，提出了新的隔离级别SSI，并且应用在系统中，使用的是图检测的方式。

分布式事务纷繁复杂，但是这里我主要讲发号器。它是整个事务的灵魂数据。一个准确，HA的发号器对于事务是非常有用的，无论是判断数据可见性，一致性，化解数据冲突等等。而分布式架构带来新的难点，因为每台机器时间不一样，会有time drift的现象。因此这鼓励学术界提出更好的发号器。后来出现了[TSO](https://www.cs.princeton.edu/courses/archive/fall10/cos597B/papers/percolator-osdi10.pdf)，逻辑时钟，[HLC](https://cse.buffalo.edu/~demirbas/publications/hlc.pdf)，True Time etc.。HLC既能通过逻辑时钟部分的设计来减少对中心化授时的需求，也能对所有的时间戳进行排序。多点授时和多时间源的设计也适合跨区域范围部署的分布式数据库，成为大部分分布式系统设计的首选。True Time是从硬件层面做到误差控制。不同数据中心之间可以通过GPS校验来做误差校准。


### 优化
我一直觉得优化器是整个数据库工程难度最大的部分之一。在分布式架构下，这种查询优化变得更加难了，分布式查询器有了无限的改进空间。因为分布式了，那么传统单机的分析方法很可能不准确了－－不同节点数据交互要考虑网络因素，A,B上２个表要join，是A传给B好呢，还是B传给A好？越复杂的分析，网络交互就越多，也越难以分析。

### 高可用
数据库做高可用主要有这么几种办法：
1. 硬件冗余（多台机器，比如主备）
2. 软件冗余（多副本，两地三中心，异地多活，数据复制）
3. 自动诊断，预测（用机器学习的办法来监控数据库应用的情况）

## 硬件

### CPU
目前由于硬件的限制，机器单核能力提升不大，不过数量越来越多（VLDB 2017）。多核CPU反而带来性能下降，因为不同的核会争抢相同数据资源。CPU主要有３种架构，SMP，NUMA，MPP。其中NUMA架构下如何管理多核对同一块内存区域的访问也是一个重要问题。


### 内存

Pavlo(inmemory index)
https://www.cs.cmu.edu/~pavlo/papers/zhang-sigmod2016.pdf

### Disk-NVM
之前的存储器叫闪存，flash memory。闪存分很多种，其中一种叫做NAND闪存，NAND Flash在U盘，存储卡，固态硬盘上都可以看到。U盘和SSD是加了控制器的闪存，而影响闪存速度的是控制器的性能，比如SSD性能高于U盘，主要靠的不是闪存的速度而是控制器的实现。

SRAM和DRAM的区别大家都清楚。SRAM是用锁存来存储信息，不需要刷新。DRAM是用电容电荷来存储。PCM指的是Phase-Change Memory，中文是相变化存储器，是一种非易失性存储器。

3D Xpoint就是一种NVM技术，Intel基于这个技术提出了Optane，美光科技提出了QuantX。而且是ByteAddressable的

之前由于磁盘的读取方式问题，数据库在设计数据结构的时候，也有一些兼容性的设计。比如B-Tree的每个节点是一个Page。这样其实会带来一些问题，比如并发控制怎么做？总不能写一条Record一个Page都被锁住。现在ByteAddressable类磁盘的出现，也让我们开始思考抛弃Page的设计，直接存Record是不是一种方案？（Page->Record）而且因为NVM的出现，我们也可以思考存储器的架构，比如Memory -> NVM -> Disk这样新的存储架构，带来新的思考点：为了保证读写效率，我数据库的Log要放在哪个位置？Index要放在哪个位置？传统的Write Ahead Log是不是要换成Write Back Log。

新的硬件也带来了性能上的提升。NVM让RTO更短，恢复数据更快，Index表查找更快。

### Network
RDMA，直接访问对方内存(Bypass CPU)。这项技术是分布式领域非常关键的技术，它对于2PC的设计有着非常深远的影响。计算数据能在网卡层面做过滤，而不需要中断CPU进行参与。

新的网络协议（QPCK，HTTP 2）给需要频繁RPC的分布式数据库提供了新的机遇。


## Deployment 
云原生数据库里面最火的明星产品可以说是Amazon的Aurora。它的细节这里不讲，有兴趣可以看看另一篇文章。它的主要设计理念是降低I/O，异步写Page，并且把这个task offload到内部存储系统。存储计算解耦，可以做到分层扩容。比如说计算节点增加500个，存储节点只增加100个，按需分配。同时它的架构也是一写多读的。

云原生数据库有几个研究点: 

* Near Data Processing。既然存储计算完全分离，这会加重一些新的问题，比如查询得到的大量数据，如果不在存储层进行处理，而是全部传输给计算节点，这会极大地浪费网络I/O。因此，现在计算分离的架构里面，通常都会有模块做算子的下推。TiKV里负责下推的是Coprocessor模块。这方面的难点是，底层的KV存储没有完整的关系表信息，因此在做一些优化的时候无能为力。


* 支持多写。在云原生数据库里支持多写是比较难的一件事情，比如Aurora就不支持。新的PolarDB是支持的


* 内存虚拟化，把内存做高可用，通过RDMA连接。这个可以联想到PolarDB 1.0。



## Data Model
Time Series　(时空系列数据库)也是最近研究的一个热点。它的数据模型主要以时间戳开头，并且写读比大概是99:1（写多读少）。主要用来存储时刻变化的数据，比如某地的风向。互联网应该是普遍被用来存trace数据。

Graph （图数据库），通过图模型来表示关系。理论上图模型可以表示各种关系，但由此也带来了很大的计算量。

