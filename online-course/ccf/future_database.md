# 数据库未来可能的研究方向

很多人觉得数据库发展这么久，理论如此成熟，科研里面可以做的东西少了。其实不然，在当下数据越来越爆炸的时代，挑战与机会是并存的。本文是对Guoliang Li老师在CCF走进校园的报告的一份总结，供个人学习。

先回顾一下数据库的架构。从单体数据库，到主备来做高可用，再到Oracle RAC的多活，再到完全分布式架构的数据库。RAC这种类型支持了多写，但是却在存储容量上受限，并且有一个很重要的问题(cache一致性)。cache一致性是指，更新操作一般是WAL+内存来避免同步随机写，那这样一来就带来一个问题，A可能把更新数据存在内存里了，还没来得及写入磁盘，但是B这时候收到一个读相关数据的请求，要怎么样保证能读到最新的数据（最新的数据在A的内存里）？



## 分布式数据库的挑战

### 分片(28:18)
分布式数据库，一个很重要的关键点就是数据怎么分的问题。行存还是列存？这个比较好选择。有更多问题等着研究，比如

* 如何保证你选的partition key在数据增长的时候依然保持均匀分布？
* 如何选择Partition的列？是不是应该在Query的比较多的列上做Partition？
* Join的比较频繁的2个Region，是不是一般放到一个节点比较好？（跨Region查询）

这里有人在[SIGMOD](https://dl.acm.org/doi/10.1145/3318464.3389704)上用学习的办法去学partition应该怎么做。同样还有Graph Key Partition。这里没听的特别明白，不过我觉得确实也大有可为，比如你本身就是Graph Database，那么用Graph Key Partition也就很符合数据特性。或者说你用图的办法建模，例如每个节点表示一列，这样可以把频繁Join或者做事务的2个Region放到一个节点上。

以及还有一个研究关键点，partiton完了之后，怎么Evaluate我的partition方法好不好？因为很有可能分片的方案非常多，不可能做到每一种新的Partition都完整跑SQL Query来评测。因此有没有一种Evaluate Model，我给出一个分片方案，就能以比较数学的方式来评估方案的表现？我觉得如果有了这么一个优秀模型，对于[Automatically Partition](https://hstore.cs.brown.edu/papers/hstore-partitioning.pdf)领域应该是一个非常好的助推剂。

### 事务(34:20)
分布式的事务支持，是Newsql和分库分表方案的关键点。有很多业务至今没有使用Newsql，就是因为它们的事务模型比较简单，分库分表就能解决。因此，这个模块怎么设计也是一大难题。

业界普遍使用的比较多的模型是2PC。但是2PC有很多种实现方式，比如各种优化。事务模型涉及到的概念比较多，比如怎么做并发控制，怎么控制写写冲突（一般通过锁），读写冲突（MVCC）。同时，数据库要支持哪些隔离级别也非常影响相关的设计。比较传统的方案，应该是通过统一发号器，每次查询前/事务开始前拿到一致性视图。PG在08年的[论文](https://courses.cs.washington.edu/courses/cse444/08au/544M/READING-LIST/fekete-sigmod2008.pdf)，提出了新的隔离级别SSI，并且应用在系统中，使用的是图检测的方式。

分布式事务纷繁复杂，但是这里我主要讲发号器。它是整个事务的灵魂数据。一个准确，HA的发号器对于事务是非常有用的，无论是判断数据可见性，一致性，化解数据冲突等等。而分布式架构带来新的难点，因为每台机器时间不一样，会有time drift的现象。因此这鼓励学术界提出更好的发号器。后来出现了[TSO](https://www.cs.princeton.edu/courses/archive/fall10/cos597B/papers/percolator-osdi10.pdf)，逻辑时钟，[HLC](https://cse.buffalo.edu/~demirbas/publications/hlc.pdf)，True Time etc.。HLC既能通过逻辑时钟部分的设计来减少对中心化授时的需求，也能对所有的时间戳进行排序。True Time是从硬件层面做到误差控制。不同数据中心之间可以通过GPS校验来做时钟误差校准。


### 优化(39:49)
我一直觉得优化器是整个数据库工程难度最大的部分之一。在分布式架构下，这种查询优化变得更加难了，分布式查询器有了无限的改进空间。因为在分布式的环境下了，传统单机的分析方法很可能不适用了－－不同节点数据交互要考虑网络因素，A,B上２个表要join，是A传给B好呢，还是B传给A好？越复杂的分析，网络交互就越多，也越难以分析。

### 高可用(41:18)
这一部分主要是业界研究的比较多，学术界因为没有相关场景所以比较难做。
数据库做高可用主要有这么几种办法：
1. 硬件冗余（多台机器，比如主备）
2. 软件冗余（多副本，两地三中心，异地多活，数据复制）
3. 自动诊断，预测（用机器学习的办法来监控数据库应用的情况）

## 硬件(44:03)

硬件一直在影响着数据库的设计。想想我们为什么会有WAL？是为了把随机写开销化成顺序写。为什么会有buffer pool？为了写操作和读操作更加迅速。为什么我们要用Page作为B+ Tree的节点单位？为了简化磁盘读取方式。新的硬件出现，势必会影响数据库的设计，而这带来了新的研究热点。

### CPU(44:03)
目前由于硬件的限制，机器单核能力提升不大，不过数量越来越多。多核CPU反而带来性能下降，因为不同的核会争抢相同数据资源。CPU主要有３种架构，SMP，NUMA，MPP。其中NUMA架构下如何管理多核对同一块内存区域的访问也是一个重要问题，因为NUMA架构允许跨节点的数据访问，因此如何控制对同一块数据资源的并发问题，或者说设计一个在重核情况下的新的[并发控制的算法](http://vldb.org/pvldb/vol10/p49-wang.pdf)，就显得尤为重要。


### NVM(49:08)
> 之前的存储器叫闪存，flash memory。闪存分很多种，其中一种叫做NAND闪存。NAND闪存在U盘，存储卡，固态硬盘上都可以看到。U盘和SSD是加了控制器的闪存，而影响闪存速度的是控制器的性能，比如SSD性能高于U盘，主要靠的不是闪存的速度而是控制器的实现。U盘和SSD都相当于是基于电的存储。
SRAM和DRAM的区别大家都清楚。SRAM是用锁存来存储信息，不需要刷新。DRAM是用电容电荷来存储。PCM指的是Phase-Change Memory，中文是相变化存储器，是一种非易失性存储器。

3D Xpoint就是一种NVM技术，Intel基于这个技术提出了Optane，美光科技提出了QuantX。而且是ByteAddressable的。ByteAddressable这个概念+ NVM和普通NAND的读写差异，给了数据库研究人员很多想象空间。比如之前我往磁盘上读写的时候用的是Page，现在有了和内存一样的ByteAddressable功能，那我可不可以换成Record？降低数据粒度。

而且因为NVM的出现，我们也可以思考存储器的架构，比如Memory -> NVM -> Disk这样新的存储架构，带来新的思考点：
* Log要不要放NVM？RTO会不会快很多？
* Index要不要放NVM？索引会不会快很多？
* 传统的Write Ahead Log是不是要换成[Write Back Log](http://www.vldb.org/pvldb/vol10/p337-arulraj.pdf)? 


### Network(55:49)
RDMA，直接访问对方内存(Bypass CPU)。这项技术是分布式领域非常关键的技术，它又会带来哪些新研究方向呢？

* 事务。2PC避免不了网络交互，RDMA能极快提升事务处理能力。
* Offloading。解放CPU，不被频繁中断。
* Programmable。网卡上直接过滤（网卡可以过滤，不需要CPU参与）
* Protocol。替换传统TCP/IP协议。

## Deployment 
云原生数据库里面曾经最火的明星产品可以说是Amazon的Aurora。它的细节这里不讲，有兴趣可以看看另一篇[文章](https://zhuanlan.zhihu.com/p/186286403)。它的主要设计理念是降低I/O，异步写Page，并且把这个task offload到内部存储系统。存储计算解耦，可以做到分层扩容。比如说计算节点增加500个，存储节点只增加100个，按需分配。同时它的架构也是一写多读的。

云原生数据库有几个研究点: 

* Near Data Processing。既然存储计算完全分离，这会加重一些新的问题，比如查询得到的大量数据，如果不在存储层进行处理，而是全部传输给计算节点，这会极大地浪费网络I/O。因此，现在计算分离的架构里面，通常都会有模块做算子的下推。TiKV里负责下推的是Coprocessor模块。这方面的难点是，底层的KV存储没有完整的关系表信息，因此在做一些优化的时候无能为力。


* 支持多写。在云原生数据库里支持多写是比较难的一件事情，比如Aurora就不支持。新的PolarDB是支持的。


* 内存虚拟化，把内存做高可用，通过RDMA连接。这个可以联想到PolarDB 1.0。
