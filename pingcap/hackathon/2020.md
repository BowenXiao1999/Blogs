# TiDB 2020 Hackathon 项目速览

本文的目的在于用简短的几句话通俗地解释各个项目要做啥。

## Xteam - 基于Region级别的follower read优化
TiKV有个选项叫做follower read，开启后能使用副本分担读请求（之前的读写请求都要由leader负责）。它的作用域为每个session，也就是说开启后，当前client的读请求可以打到follower上。但是，follower read并不是零成本的，因为在实习上每次follower read发起，follower都需要发一次RPC向leader节点询问当前的执行进度，这会产生一次额外的网络请求开销。这同样带来一个问题：是否同一个Session的所有Region都适合开启follower read？如果我们增加对某部分热点Region的follower Read选项，是否会增强数据库的性能？

## TiFS
一个用户态文件系统(FUSE)。传统的文件系统都是用内核态的code写的，调试比较麻烦。现在，我们可以基于libfuse([C library](https://github.com/libfuse/libfuse/))，把打到内核态的函数调用(比如open一个文件，传统办法会进入内核执行，但是利用这个库，内核线程只传递信号给用户态线程，提醒它去执行对应操作)转化成用户态的文件操作调用，所以用户只需编写用户态的代码即可实现文件系统。这个组用Rust写的，基于一个rs版的[libfuse] (https://github.com/cberner/fuser)。结合之前的介绍，通过这个库，就可以在用户态调用TiKV的api来实现用户态的分布式存储系统。

## ' or 0=0 or ' (UDF)

> 用户可使用 Golang、Rust、C、C++ 等语言编写自定义逻辑的程序，使用我们提供的工具链转编译到 WebAssembly 字节码后，通过以下方式加载到 TiDB 数据库：
```
CREATE FUNCTION [name] WASM_BYTECODE [bytecode_binary];
```

TiDB/TiKV收到bytecode_binary后，使用LLVM等技术编译为当前平台原生指令后执行。同时这些指令还将缓存在本地，提升性能。不仅仅实现了功能，还做了下推的优化。TiDB在给TiKV下推UDF字节码时，先下推ID，若TiKV本地没有则重新下推完整字节码。TiKV 收到字节码后，将字节码存储在本地并编译，后续不再需要该函数的字节码。

感觉RFC写的很详细。

## TiGraph
这个项目非常好理解，本质上是希望基于底层的TiKV存储，扩展TiDB的图查询能力。他们做的时候，没有用TiKV这种生产级别的存储引擎，使用的是Unistore实现Key-Value。即使是这样，对于4度人脉的workload的查询也提高了8000+倍。。看来图模型在specific workload下真的有很好的表现。另外他们测试的N度人脉，指的是一张图中间任意2个节点可以用小于等于N条边联通。

他们的一个贡献是实现了图查询语义。传统的图查询，比如查询某位推特用户(id=1234)的关注者的关注者的关注者，这个要写非常复杂的SQL查询（见答主截图）。如果用图语义的话，非常简洁明了，减少编写出错的可能。感觉他们找到了很好的支点去复用整个TiDB的能力，看得出对于TiDB内核模块比较了解，因此这个feature的融合显得非常顺滑。他们支持的feature非常多，有算子下推，索引表，甚至还有事务（做好了算是解决了一个痛点吧？原来业务上的图查询往往不能和关系型查询做到一个事务里，现在不用单独用其他图DB了）

## GPU Accelerated TiDB For Analytical Queries
思路是比较好理解的，就是首先观察到TiDB的CPU在某些算子执行(Join, Aggregate)时，速度比较慢，希望把intensive的计算挪到GPU上来加速。因为TiDB是Go写的，想用CUDA得是C++/C，这就需要CGO了。但是完全用CGO来实现一遍算子执行逻辑难度比较高，于是他们结合Thrust (封装一些并行化的GPU上的操作，比如Sort) + cuRF (处理GPU上数据的库，有点像GPU的Pandas(DataFrames))。他们基于这几个库做了一个C++版的GPU Relation Algebra （能在GPU上执行算子的C++动态库），然后link进Go的process，用CGO去调用。

这里还有一个细节，就是Go里存储数据的Chunk是放在Memory里并且Arrow(一种Columnar Format) Compatitable的，这样子feed进GPU Relation Algebra的时候可以做到zero-copy cost。而且由于只实现了部分算子的GPU加速，所以一个Query Plan是一部分由CPU计算，一部分由GPU计算。
